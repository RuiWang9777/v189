---
title: Sliced Wasserstein variational inference
crossref: acml22
abstract: "Variational Inference approximates an unnormalized\r distribution via the
  minimization of\r Kullback-Leibler (KL) divergence. Although this\r divergence is
  efficient for computation and has been\r widely used in applications, it suffers
  from some\r unreasonable properties. For example, it is not a\r proper metric, i.e.,
  it is non-symmetric and does\r not preserve the triangle inequality. On the other\r
  hand, optimal transport distances recently have\r shown some advantages over KL
  divergence. With the\r help of these advantages, we propose a new\r variational
  inference method by minimizing sliced\r Wasserstein distanceâ€“a valid metric arising
  from\r optimal transport. This sliced Wasserstein distance\r can be approximated
  simply by running MCMC but\r without solving any optimization problem. Our\r approximation
  also does not require a tractable\r density function of variational distributions
  so\r that approximating families can be amortized by\r generators like neural networks.
  Furthermore, we\r provide an analysis of the theoretical properties of\r our method.
  Experiments on synthetic and real data\r are illustrated to show the performance
  of the\r proposed method."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yi23a
month: 0
tex_title: Sliced Wasserstein variational inference
firstpage: 1213
lastpage: 1228
page: 1213-1228
order: 1213
cycles: false
bibtex_author: Yi, Mingxuan and Liu, Song
author:
- given: Mingxuan
  family: Yi
- given: Song
  family: Liu
date: 2023-04-13
address:
container-title: "Proceedings of The 14th Asian Conference on Machine\r Learning"
volume: '189'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 13
pdf: https://proceedings.mlr.press/v189/yi23a/yi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
