---
title: "A Novel Differentiable Mixed-Precision Quantization\r Search Framework for
  Alleviating the Matthew Effect\r and Improving Robustness"
crossref: acml22
abstract: "Network quantization is an effective and widely-used\r model compression
  technique. Recently, several works\r apply differentiable neural architectural search\r
  (NAS) methods to mixed-precision quantization (MPQ)\r and achieve encouraging results.
  However, the nature\r of differentiable architecture search can lead to\r the Matthew
  Effect in the mixed-precision. The\r candidates with higher bit-widths would be
  trained\r maturely earlier while the candidates with lower\r bit-widths may never
  have the chance to express the\r desired function. To address this issue, we propose\r
  a novel mixed-precision quantization framework. The\r mixed-precision search is
  resolved as a distribution\r learning problem, which alleviates the Matthew\r effect
  and improves the generalization\r ability. Meanwhile, different from generic\r differentiable
  NAS methods, search space will grow\r rapidly as the depth of the network increases
  in the\r mixed-precision quantization search. This makes the\r supernet harder to
  train and the search process\r unstable. To this end, we add a skip connection with\r
  a gradually decreasing architecture weight between\r convolutional layers in the
  supernet to improve\r robustness. The skip connection will help the\r optimization
  of the search process and will not\r participate in the bit width competition. Extensive\r
  experiments on CIFAR-10 and ImageNet demonstrate the\r effectiveness of the proposed
  methods. For example,\r when quantizing ResNet-50 on ImageNet, we achieve a\r state-of-the-art
  156.10x Bitops compression rate\r while maintaining a 75.87$%$ accuracy."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou23a
month: 0
tex_title: "A Novel Differentiable Mixed-Precision Quantization\r Search Framework
  for Alleviating the Matthew Effect\r and Improving Robustness"
firstpage: 1277
lastpage: 1292
page: 1277-1292
order: 1277
cycles: false
bibtex_author: Zhou, Hengyi and He, Hongyi and Liu, Wanchen and Li, Yuhai and Zhang,
  Haonan and Liu, Longjun
author:
- given: Hengyi
  family: Zhou
- given: Hongyi
  family: He
- given: Wanchen
  family: Liu
- given: Yuhai
  family: Li
- given: Haonan
  family: Zhang
- given: Longjun
  family: Liu
date: 2023-04-13
address:
container-title: "Proceedings of The 14th Asian Conference on Machine\r Learning"
volume: '189'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 13
pdf: https://proceedings.mlr.press/v189/zhou23a/zhou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
