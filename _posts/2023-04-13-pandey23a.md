---
title: On the Interpretability of Attention Networks
crossref: acml22
abstract: "Attention mechanisms form a core component of\r several successful deep
  learning architectures, and\r are based on one key idea: “The output depends only\r
  on a small (but unknown) segment of the input.” In\r several practical applications
  like image captioning\r and language translation, this is mostly true. In\r trained
  models with an attention mechanism, the\r outputs of an intermediate module that
  encodes the\r segment of input responsible for the output is often\r used as a way
  to peek into the ‘reasoning’ of the\r network. We make such a notion more precise
  for a\r variant of the classification problem that we term\r selective dependence
  classification (SDC) when used\r with attention model architectures. Under such
  a\r setting, we demonstrate various error modes where an\r attention model can be
  accurate but fail to be\r interpretable, and show that such models do occur as\r
  a result of training. We illustrate various\r situations that can accentuate and
  mitigate this\r behaviour. Finally, we use our objective definition\r of interpretability
  for SDC tasks to evaluate a few\r attention model learning algorithms designed to\r
  encourage sparsity and demonstrate that these\r algorithms help improve interpretability."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pandey23a
month: 0
tex_title: On the Interpretability of Attention Networks
firstpage: 832
lastpage: 847
page: 832-847
order: 832
cycles: false
bibtex_author: Pandey, Lakshmi Narayan and Vashisht, Rahul and Ramaswamy, Harish G.
author:
- given: Lakshmi Narayan
  family: Pandey
- given: Rahul
  family: Vashisht
- given: Harish G.
  family: Ramaswamy
date: 2023-04-13
address:
container-title: "Proceedings of The 14th Asian Conference on Machine\r Learning"
volume: '189'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 13
pdf: https://proceedings.mlr.press/v189/pandey23a/pandey23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v189/pandey23a/pandey23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
